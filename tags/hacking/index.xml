<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hacking on norsey&#39;s hideout</title>
    <link>http://norsec0de.github.io/tags/hacking/</link>
    <description>Recent content in hacking on norsey&#39;s hideout</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 04 Nov 2012 01:26:05 +0000</lastBuildDate><atom:link href="http://norsec0de.github.io/tags/hacking/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>finding the apache log files using burp intruder</title>
      <link>http://norsec0de.github.io/posts/finding-the-apache-log-files-using-burp-intruder/</link>
      <pubDate>Sun, 04 Nov 2012 01:26:05 +0000</pubDate>
      
      <guid>http://norsec0de.github.io/posts/finding-the-apache-log-files-using-burp-intruder/</guid>
      <description>Often when conducting security assessments it is necessary to go beyond just identifying the vulnerability, reporting it and heading out for a beer. Sometimes, like when conducting a penetration test or when asked by a client to demonstrate business risk, it is necessary to gain command line line access to the machine to show the risks associated with having a web user being able to execute commands on their machine. Often this involves getting a shell by some means but in the case of Local File Inclusion (LFI) simply finding the Apache Log location folder can be enough to start running commands on the system as the Apache service account.</description>
    </item>
    
    <item>
      <title>is robots.txt dead? lets ask dropbox</title>
      <link>http://norsec0de.github.io/posts/is-robots-txt-dead-lets-ask-dropbox/</link>
      <pubDate>Mon, 17 Sep 2012 09:48:35 +0000</pubDate>
      
      <guid>http://norsec0de.github.io/posts/is-robots-txt-dead-lets-ask-dropbox/</guid>
      <description>First off I&amp;rsquo;d like to give creds to Francis Brown and Rob Ragan who presented their talk Tenacious Diggity at DEFCON 20Â where I found out about the apparent steam-rolling of Dropbox&amp;rsquo;s robots.txt file. For as far back as I can remember, the robots.txt file has been a ban-list of places that search engine crawlers are supposed to ignore when crawling a site. Recently however there is some talk that the preferred way of disallowing crawlers is to control them using alternative methods such as metatags and javascript.</description>
    </item>
    
  </channel>
</rss>
